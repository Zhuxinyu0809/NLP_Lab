{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6bacf2-48ff-47f2-b27a-49da5abcb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "481e3bca-fc98-4df6-a9c3-244513c429f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b9b82b-8a22-48d6-8b7a-bdf8e1526a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/jujusmacbook/Documents/NLP_Lab/Data/train.csv')\n",
    "X = train_df[\"review\"]\n",
    "y = train_df[\"sentiment\"]\n",
    "\n",
    "# 划分训练集与验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  # 分层抽样保证标签分布一致\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc933f4b-b72b-40e6-b017-5b02e4bbf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2025.9.18-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2025.9.18-cp313-cp313-macosx_11_0_arm64.whl (287 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [nltk][32m3/4\u001b[0m [nltk]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.0 nltk-3.9.2 regex-2025.9.18 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b17de6-2466-4765-a2ec-04e354eb1bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jujusmacbook/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # 保留Top5000高频词\n",
    "    stop_words=stopwords.words('english'),  # 去除停用词\n",
    "    ngram_range=(1, 2)  # 保留1-gram（单词）与2-gram（词组，如“waste time”）\n",
    ")\n",
    "\n",
    "# 拟合训练集并转换特征\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)  # 测试集仅转换，避免数据泄露"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3390f804-7462-4743-a263-fe1c77f04b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集准确率：1.0000\n",
      "分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       160\n",
      "           1       1.00      1.00      1.00       160\n",
      "\n",
      "    accuracy                           1.00       320\n",
      "   macro avg       1.00      1.00      1.00       320\n",
      "weighted avg       1.00      1.00      1.00       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 初始化逻辑回归模型\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,  # 增加迭代次数确保收敛\n",
    "    class_weight='balanced'  # 平衡正负样本（虽数据集平衡，仍增加鲁棒性）\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 验证集预测\n",
    "y_val_pred = lr_model.predict(X_val_tfidf)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"验证集准确率：{val_accuracy:.4f}\")\n",
    "print(\"分类报告：\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80e920a-4e36-48b9-8a0a-5b7edfbd357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试集\n",
    "test_df = pd.read_csv(\"/Users/jujusmacbook/Documents/NLP_Lab/Data/test.csv\")\n",
    "X_test = test_df[\"review\"]\n",
    "\n",
    "# 转换测试集特征\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# 预测情感标签\n",
    "test_df[\"sentiment\"] = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# 生成提交文件（按要求命名：NAME_STUDENT_ID_predictions.csv）\n",
    "submission = test_df[[\"id\", \"sentiment\"]]\n",
    "submission.to_csv(\"ZHU_Xinyu_25118165g_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370dfc68-ad34-469c-934d-80d818449125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
