{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6c4937-a79b-4e96-8868-fa0630c29367",
   "metadata": {},
   "source": [
    "ËØçË¢ãÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496629f-1c5e-4f71-bf18-38adc9afb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "# Ignore version-related warnings (no errors in original code, retain this logic)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------- 1. Data Loading \n",
    "data_train = pd.read_csv('/Users/jujusmacbook/Documents/NLP_Lab/Data/train.csv')\n",
    "data_test = pd.read_csv('/Users/jujusmacbook/Documents/NLP_Lab/Data/test.csv')\n",
    "\n",
    "# Data exploration \n",
    "print(\"Training Set Basic Info:\")\n",
    "print(f\"Training set shape: {data_train.shape} (1600 rows, including 'id', 'review', 'sentiment' columns)\")\n",
    "print(f\"Training set sentiment label distribution:\\n{data_train['sentiment'].value_counts(normalize=True).round(4)}\")  # Verify data balance (üî∂3-12)\n",
    "print(\"\\nTest Set Basic Info:\")\n",
    "print(f\"Test set shape: {data_test.shape} (5001 rows, including 'id', 'review' columns)\")\n",
    "\n",
    "# ---------------------- 2. Data Preprocessing: Extract Text and Labels \n",
    "train_sentences = data_train['review']\n",
    "label = data_train['sentiment']  # 1600 sentiment labels (1 = positive, 0 = negative)\n",
    "\n",
    "# Extract text ('review') from test set; Note: Original code mistakenly used train_sentences here, corrected to data_test['review'] (key correction)\n",
    "test_sentences = data_test['review']\n",
    "\n",
    "# Concatenate training and test set text (for Bag-of-Words model fitting, avoid unseen words in test set)\n",
    "sentences = pd.concat([train_sentences, test_sentences])\n",
    "print(f\"\\nCombined corpus size: {sentences.shape} (3200 rows total, including train + test text)\")\n",
    "\n",
    "# ---------------------- 3. Text Feature Engineering: Bag-of-Words Model Construction \n",
    "stop_words = open('/Users/jujusmacbook/Documents/NLP_Lab/Data/stop_words.txt', encoding='utf-8').read().splitlines()\n",
    "print(f\"\\nStopword list size: {len(stop_words)} stopwords (e.g., 'the', 'and', etc.)\")\n",
    "\n",
    "# Build Bag-of-Words model (CountVectorizer), no errors in original code, retain core parameters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "co = CountVectorizer(\n",
    "    analyzer='word',          # Split text by \"word\" (suitable for Latin languages)\n",
    "    ngram_range=(1, 2),       # Retain unigrams (e.g., 'good') and bigrams (e.g., 'very good') to mitigate word order loss\n",
    "    stop_words=stop_words,    # Incorporate stopword filtering\n",
    "    max_features=5000         # Retain high-frequency words to avoid feature dimensionality explosion\n",
    ")\n",
    "\n",
    "# Fit Bag-of-Words model with combined corpus \n",
    "co.fit(sentences)\n",
    "print(f\"Bag-of-Words model vocabulary size: {len(co.vocabulary_)} feature words (top 5000 high-frequency words retained)\")\n",
    "\n",
    "# ---------------------- 4. Train Set Splitting: Divide into Training Subset and Validation Subset (Meets Document üî∂3-22 \"Analyze Validation Set Performance\") ----------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split training set into \"training subset (80%)\" and \"validation subset (20%)\", ensure consistent label distribution (stratify=label)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    train_sentences, label, random_state=1234, stratify=label\n",
    ")\n",
    "print(f\"\\nSplit sizes: Training subset {x_train.shape}, Validation subset {x_test.shape}\")\n",
    "\n",
    "# Convert features with Bag-of-Words model (training subset + validation subset)\n",
    "x_train_bow = co.transform(x_train).toarray()  # Feature transformation for training subset\n",
    "x_test_bow = co.transform(x_test).toarray()    # Feature transformation for validation subset\n",
    "print(f\"Bag-of-Words feature dimensions: Training subset {x_train_bow.shape}, Validation subset {x_test_bow.shape}\")\n",
    "\n",
    "# ---------------------- 5. Model Training: Baseline Model Comparison \n",
    "# 5.1 Logistic Regression Model (Baseline Version)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg1 = LogisticRegression()\n",
    "lg1.fit(x_train_bow, y_train)\n",
    "lg1_acc = lg1.score(x_test_bow, y_test)\n",
    "print(f\"\\nBaseline Logistic Regression (Bag-of-Words features): Validation set accuracy {lg1_acc:.4f}\")\n",
    "\n",
    "# 5.2 Naive Bayes Model (for comparison)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train_bow, y_train)\n",
    "nb_acc = classifier.score(x_test_bow, y_test)\n",
    "print(f\"Naive Bayes Model (Bag-of-Words features): Validation set accuracy {nb_acc:.4f}\")\n",
    "\n",
    "# ---------------------- 6. Model Optimization: Logistic Regression Hyperparameter Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define hyperparameter search range (regularization strength C, duality dual)\n",
    "param_grid = {\n",
    "    'C': range(1, 10),        # Larger C = weaker regularization\n",
    "    'dual': [True, False]     # Dual form (suitable for high-dimensional features)\n",
    "}\n",
    "\n",
    "# Initialize Grid Search (3-fold cross-validation, use all CPU cores for acceleration)\n",
    "lgGS = LogisticRegression()\n",
    "grid = GridSearchCV(lgGS, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "# Train Grid Search model with Bag-of-Words features (based on training subset)\n",
    "grid.fit(x_train_bow, y_train)\n",
    "\n",
    "# Output optimal hyperparameters and corresponding validation set accuracy\n",
    "print(f\"\\nGrid Search optimal hyperparameters: {grid.best_params_}\")\n",
    "lg_final = grid.best_estimator_  # Get Logistic Regression model with optimal hyperparameters\n",
    "lg_final_acc = lg_final.score(x_test_bow, y_test)\n",
    "print(f\"Optimal Logistic Regression (Bag-of-Words features): Validation set accuracy {lg_final_acc:.4f}\")\n",
    "\n",
    "# ---------------------- 7. Test Set Prediction: Generate Sentiment Labels \n",
    "# Convert test set text features with Bag-of-Words model (consistent with training process)\n",
    "test_X_bow = co.transform(data_test['review']).toarray()\n",
    "print(f\"\\nTest set Bag-of-Words feature dimensions: {test_X_bow.shape} (5001 rows, matching test set size)\")\n",
    "\n",
    "# Predict test set sentiment labels with optimal Logistic Regression model\n",
    "predictions = lg_final.predict(test_X_bow)\n",
    "print(f\"Test set prediction result size: {predictions.shape} (5001 sentiment labels, 0 = negative, 1 = positive)\")\n",
    "\n",
    "# ---------------------- 8. Generate Submission File \n",
    "# Add predicted labels to test set\n",
    "data_test.loc[:, 'sentiment'] = predictions\n",
    "# Extract \"id\" and \"sentiment\" columns as required by the document to generate final submission data\n",
    "final_data = data_test.loc[:, ['id', 'sentiment']]\n",
    "\n",
    "# View first 5 rows of submission file (verify format)\n",
    "print(f\"\\nFirst 5 rows of final submission file:\")\n",
    "print(final_data.head())\n",
    "\n",
    "# Save as CSV file\n",
    "final_data.to_csv('/Users/jujusmacbook/Documents/NLP_Lab/Data/Juju_StudentID_predictions.csv', index=False)\n",
    "print(f\"\\nSubmission file saved: Zhu Xinyu_25118165g_predictions.csv \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
